# Adversarial_robustness
Betel leaf diseases using a hybrid CNNâ€“LSTM model and evaluates robustness against adversarial attacks. Adversarial training with PGD improves both accuracy and resilience, highlighting the need for robust deep learning methods in reliable plant disease diagnosis.


<img width="505" height="355" alt="Screenshot_416" src="https://github.com/user-attachments/assets/36b4b8c4-8e72-4491-b11c-cc5cf4e68e94" />
<img width="504" height="361" alt="Screenshot_417" src="https://github.com/user-attachments/assets/240af22b-3e79-4ea4-bae1-f13e9213ab64" />
<img width="1221" height="236" alt="Screenshot_419" src="https://github.com/user-attachments/assets/842bb8a2-b845-4f8e-8f7d-7371859ed17c" />


# Adversarial Robustness over Betel Leaf Dataset

This project explores the vulnerability and robustness of deep learning models applied to a Betel Leaf classification dataset. It specifically implements adversarial attacks and evaluates model performance under perturbed conditions.

## ğŸš€ Features
* **Custom Dataset:** Processing and augmentation of Betel Leaf imagery.
* **Model Training:** Implementation of CNN-based architectures for classification.
* **Adversarial Attacks:** Implementation of techniques (e.g., FGSM, PGD) to test model security.
* **Robustness Evaluation:** Comparative analysis of accuracy on clean vs. adversarial data.

## âš™ï¸ Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/Thaharim-dev/Betel-Leaf-Robustness.git
   cd Betel-Leaf-Robustness.


## ğŸ“Š Results

The notebook provides detailed visualizations of:

1. Training/Validation loss and accuracy.
2. Visual examples of adversarial perturbations.
3. Confusion matrices for model evaluation.
